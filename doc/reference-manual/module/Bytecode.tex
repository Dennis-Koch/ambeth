\SetAPI{J-C}
\section{Bytecode}
\label{module:Bytecode}
\ClearAPI
\TODO
\subsection{Features}
\begin{itemize}
	%% FEATURES START
	\item \prettyref{feature:DataObject}
	\item \prettyref{feature:ValueHolderContainer}
	\item \prettyref{feature:EntityEquals}
	\item \prettyref{feature:InterfaceImpl}
	%% FEATURES GENERATED START
	%% FEATURES END
\end{itemize}

There are two major approaches to enhance existing object logic at runtime. Both try to deal with seperation of concerns and single point of responsibility considerations:
\begin{enumerate}
	\item By Proxies - This is approach is often used in AOP or IoC bean postprocessing but has several drawbacks in other scenarios
		\begin{itemize}
			\item Generates at least an additional object (the proxy) which increases heap overhead if several thousands of objects need to be proxied (like entities for example)
			\item In most cases a second additional object (the interceptor) is created to hold some kind of state of the proxy-based logic
			\item Some scenarios can not be solved with proxies at all: e.g. object identity comparisons or commutative constraints on the \type{Object}.equals() methods\cite{com14}
			\item These issues are true in most programming languages - especially for Java \& C\#
		\end{itemize}
	\item By manipulating the base class before instantiating and using the object
		\begin{itemize}
			\item This is of course only possible if you have a control of the instantiation phase of an object
			\item This can specifically ensured by forcing the developer to use factory-services to gain access to his objects. A major example for this approach is the feature \prettyref{feature:EntityFactory}.
			\item This way there is no additional object instance making equals- or identity-hazard obsolete
		\end{itemize}
\end{enumerate}

\AMBETH{} makes intensive use of both approaches according to the required functionality and specific scenario for maximum performance \& good maintainability:
\begin{itemize}
	\item IoC beans are enhanced by proxies through the \type{BeanPostProcessor} pipeline (\prettyref{feature:BeanPostProcessor})
	\item Entities, \type{ObjRefs} or cache-internal meta-structures are scrolled through the bytecode-enhancement pipeline
\end{itemize}
Regarding the latter example it can be stated that many generic algorithms across all modules gain great performance benefits (lower memory consumption as well as higher iteration counts per time) by implementing parts of their algorithm at runtime.

\tip{\AMBETH{} does not rely on reflection when invoking high-frequency method-calls (e.g. getter/setters of entities) or when creating entities: Instead it generates classes at runtime which then implement code to access the target getters, setters or constructors of entities.}

These generated classes are instantiated once while each implementing a specific contract interface. If \AMBETH{} e.g. needs a value from a specific getter this way it calls the ``anonymous'' \type{Member}.\typeprop{getValue} method of the corresponding generated getter. As a result the runtime performance for generic algorithms of \AMBETH{} is as high as if you would have implemented \& optimized the ``spirit'' of the whole generic algorithm solely for your specific scenario manually. In most cases the framework is indeed even faster than a developer could manually implement the corresponding complex logic by hand.

Let's take a deeper look to this ``allegation'' with a specific example:
\begin{lstlisting}[style=Java,caption={Example entity definition (Java)}]
public interface QueryEntity
{
	Integer getId();
	Short getVersion();
	String getUpdatedBy();
	String getCreatedBy();
	Date getUpdatedOn();
	Date getCreatedOn();
	String getName1();
}
\end{lstlisting}

In the example above there are 7 properties defined. Each of them is a reference type and therefore each backing field consumes 8 bytes heap on a x64 VM (for the heap pointer). Adding the 16 bytes \textit{base} overhead for each ``empty'' object on a x64 VM each instance of \type{QueryEntity} will at first take 72 bytes of heap memory for itself. In addition both Date properties consume 24 bytes each (16 \textit{base} + 8 for their long value) because they can \emph{not} be shared across entities even if they have the same internal value\footnote{That is because of their mutability: \type{java.util.Date} has a method \type{setTime} changing its state. Compared to \type{java.util.String} or boxed numbers which are each immutable.}. Last but not least the Integer instance of each \emph{id} will be mostly unique and will therefore consume additional 20 bytes for its existence (16 bytes \textit{base} + 4 bytes for its int value) though it \emph{could} be shared across entities. The \emph{version} and each of the remaining three string properties shall be assumed to be zero / null references for the sake of simplicity.\newline

The assumed deep memory consumption in a common scenario may therefore be:

\[72+2\times24+20=140\]

Let us compare this with information from a runtime heap dump:

\def\showimgref{img/visualvm-queryentity}
\showimg{Bytecode enhanced cache instance (screenshot from \textit{VisualVM})}

There may be some surprising differences:
\begin{itemize}
	\item \emph{id} is stored as a native \type{int} instead of the boxing \type{Integer}
		\begin{itemize}
			\item This reduces the memory overhead by 24 bytes: The backing field is a native 4 byte field instead of a 8 byte reference. In addition we prevent the need for a whole \type{Integer} object (20 bytes).
		\end{itemize}
	\item \emph{version} is stored as a native \type{short} instead of the boxing \type{Short}
		\begin{itemize}
			\item This reduces the memory overhead by 6 bytes: The backing field is a native 2 byte field instead of a 8 byte reference.
		\end{itemize}
	\item \emph{UpdatedOn} and \emph{CreatedOn} is stored as a native \type{long} instead of the \type{Date} instance
		\begin{itemize}
			\item This reduces the memory overhead by 24 bytes each: The backing field is a native 8 byte field instead of a 8 byte reference - so no change here. In addition we prevent the need for two \type{Date} objects (24 bytes each).
		\end{itemize}
\end{itemize}

Taking the previous 140 bytes as base we now can assume:

\[140-24-6-2\times24=62\] bytes which results in 56\% - or rounded 50\% - less memory consumption (and preventing the need of 4 of 5 objects in that example) helping against GC pressure when managing thousands to several millions of entities in memory with \AMBETH.